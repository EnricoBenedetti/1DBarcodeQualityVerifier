{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Garfield: topic extraction and retrieval system\n\n    Project and Project Work in Text Mining, Data Mining and Big Data Analytics\n    Author: Enrico Benedetti\n    Email: enrico.benedetti5 [at] studio.unibo.it\n    GitHub: https://github.com/EnricoBenedetti","metadata":{"id":"qnD9KklxFkrD","execution":{"iopub.status.busy":"2023-02-06T08:44:07.490605Z","iopub.execute_input":"2023-02-06T08:44:07.491032Z","iopub.status.idle":"2023-02-06T08:44:07.499799Z","shell.execute_reply.started":"2023-02-06T08:44:07.490999Z","shell.execute_reply":"2023-02-06T08:44:07.497467Z"}}},{"cell_type":"markdown","source":"## Download and first look at the Garfield Dataset","metadata":{"id":"OB3t489XAC-5"}},{"cell_type":"code","source":"#!pip install --upgrade numpy\n#!pip install --upgrade gensim\n!pip install gensim==4.1.0\n!pip install kneed\n!pip install sentence_transformers\n#!pip install torch==1.12.0","metadata":{"id":"htoNP9XZM831","outputId":"1574574c-dfa0-43c2-c39a-c6084f95343f","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import urllib.request\n# download the raw data online\nurllib.request.urlretrieve(\"http://john.ccac.rwth-aachen.de:8000/ftp/dilbert/garfield.txt\", \"garfield.txt\")","metadata":{"id":"tvd-1GTL1f3a","outputId":"6f67a8e3-fe43-435a-f15a-ef058b61ff86","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gensim\n#plt.rcParams[\"figure.figsize\"] = (25, 10)\n#plt.rcParams['figure.dpi'] = 450\n#plt.rcParams['axes.xmargin'] = 0\n#plt.rcParams['axes.ymargin'] = 0","metadata":{"id":"P8Y6brrp2Rop","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read text file into pandas DataFrame\ndf = pd.read_csv(\"garfield.txt\", sep=\" -- \", encoding = 'latin1', names=[\"stripID\", \"text\"], engine='python')","metadata":{"id":"9nrRkwEN2S5V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna().copy()","metadata":{"id":"PAWvaieL72__","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"Z8SbzG2o23d4","outputId":"d79eba01-adb7-494e-c884-3d2b03c52196","colab":{"base_uri":"https://localhost:8080/","height":424},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataframe setup","metadata":{"id":"u1XtAjyt4bWu"}},{"cell_type":"markdown","source":"### Adding date information to dataframe\n\nUsing the format specifications of `datetime` and `pandas`, a timestamp is extracted from the `stripID` column and added as a `Date` column.\n\nThe code below tries to avoid throwing away some strips (~10) that have parsing errors in the `StripID` column.","metadata":{"id":"WjliNKCN2FBk"}},{"cell_type":"code","source":"preformat_dates = df['stripID'].to_numpy()\nformatted_dates = preformat_dates.copy()\nfor index, stripID in enumerate(preformat_dates):\n  try:\n    formatted_dates[index] = pd.to_datetime(stripID, format=\"ga%y%m%d\")\n  except: \n    # fix abnormal month using the same as the previous strip (they are temporally ordered)\n    previous_date_month = formatted_dates[index-1].strftime('%m')\n    # avoids mistakes in the first two characters (eg. gs instead of ga)\n    stripID = 'ga' + stripID[2:4] + previous_date_month + stripID[-2:]\n    formatted_dates[index] = pd.to_datetime(stripID, format=\"ga%y%m%d\")\n    print(f'Fixed string with error {preformat_dates[index]} to correct timestamp {formatted_dates[index]}')\n    preformat_dates[index] = stripID\n\n# add the date column and also the correct stripID\ndf['Date'] = formatted_dates\ndf['stripID'] = preformat_dates","metadata":{"id":"9SMnYU6z36c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Concatenating text from the same strips together in a single dataframe row\n\nAs the .txt file the dataset is built upon contains text from the same strip on different lines, grouping by `stripID` is performed and the `text` features are concatenated using the `sum` operator for strings.","metadata":{"id":"uzGoPGEaC3M6"}},{"cell_type":"code","source":"unique_strips = len(df['stripID'].unique())\nprint(f'Unique stripIDs in dataset: {unique_strips}. Total rows in dataset: {df.shape[0]}')","metadata":{"id":"sZUxHbAKDzY-","outputId":"bf81a1cd-6f16-4da5-c5d6-65f0ef8d113f","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# groupd documents as same strip id\ndf_grouped = df.groupby(\"stripID\", as_index=False).agg({'Date': 'first', 'text':'sum'})\nprint(f'Unique stripIDs in dataset: {unique_strips}. Total rows now in dataset: {df_grouped.shape[0]}')\ndf_grouped.head()","metadata":{"id":"PhlXjSlY-5hB","outputId":"315ff49b-5bb0-4d4d-df18-c30dbcf8ca97","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sorting values by date\n\nNow the dataframe has strips ordered by date, with each strip text in a single row.","metadata":{"id":"ucRP6cvUEc5y"}},{"cell_type":"code","source":"df_sorted = df_grouped.sort_values(by='Date', ascending=True).reset_index(drop=True)\ndf_sorted","metadata":{"id":"z6dWUFne_3pM","outputId":"270c3a2f-33d9-445e-c718-736bb524d05f","colab":{"base_uri":"https://localhost:8080/","height":424},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"garfield_start = df_sorted['Date'].iloc[0]\ngarfield_current = df_sorted['Date'].iloc[-1]\ngarfield_yearspan = np.floor((garfield_current - garfield_start).days / 365.25).astype('int')\nprint(f'The Garfield comic has been around from {garfield_start.strftime(\"%B %Y\")} to {garfield_current.strftime(\"%B %Y\")}, i.e. around {garfield_yearspan} years.')","metadata":{"id":"31ePQRM6FZFa","outputId":"0470b5c9-155f-4905-c7a9-c1b5d5076ca5","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding links to the strips in dataframe\n\nJust for fun.","metadata":{"id":"22mQDE8h3jke"}},{"cell_type":"code","source":"from IPython.display import HTML # link visualization\ndef make_clickable(url, name):\n    return '<a href=\"{}\" rel=\"noopener noreferrer\" target=\"_blank\">{}</a>'.format(url,name)","metadata":{"id":"BvOKA1dh4l1-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# site url\ncomic_site_url = 'https://www.gocomics.com/garfield/'\n# Add url column to dataframe\ndf_sorted['url'] = df_sorted['Date'].apply(lambda date: comic_site_url + date.strftime('%Y/%m/%d')).apply(lambda url: make_clickable(url=url, name='Comic'))","metadata":{"id":"x2bMHv2I3uZ-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sorted.head().style","metadata":{"id":"uE5l0oUP3skE","outputId":"dc03b8c4-5584-4b42-94f1-bcae92df62cf","colab":{"base_uri":"https://localhost:8080/","height":206},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text preprocessing and dictionary creation\n\nStarting from the term document matrix. \n\n- Topic extraction using LSA.\n- Retrieval of strips given a textual query using cosine distance in LSA space.\n\nComments: The document representation approach that is used is the BoW with plain term-document frequencies. Another approach could have been one that uses positional information of a word in the strip, or also using TF-IDF values in the BoW. As the strips are made of few words, just normal term counts should be enough.","metadata":{"id":"X-YSYRAWBfto"}},{"cell_type":"code","source":"#from gensim import corpora\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, preprocess_string, strip_short, stem_text\n\n# lemmatizing over stemming\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.stem import WordNetLemmatizer\nwnlem = WordNetLemmatizer()\n\n# preprocess given text\ndef preprocess(text):\n    # clean text based on given filters\n    CUSTOM_FILTERS = [lambda x: x.lower(), \n                                remove_stopwords, \n                                strip_punctuation, \n                                strip_short, \n                                #stem_text]\n                                wnlem.lemmatize]\n    #]\n    text = preprocess_string(text, CUSTOM_FILTERS)\n    \n    return text\n\n# apply function to all reviews \ndf_sorted['tokens'] = df_sorted['text'].apply(lambda x: preprocess(x))\ndf_sorted.head()","metadata":{"id":"AIYWkPM55gtx","outputId":"567a163d-c2ed-473a-9e3f-bbffbc74f79b","colab":{"base_uri":"https://localhost:8080/","height":242},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `corpus` is the document matrix.\n\nThe `dictionary` maps each text token to a numerical id.\n\nCollection frequencies `dictionary.cfs`: token_id $→$ how many instances of this token are contained in the documents.\n\nThe `bow` (Bag of Words) contains one list per document. Each document list contains tuples of (`word index`, `n. of occurrences`).","metadata":{"id":"RUcv7Zl1IzS4"}},{"cell_type":"code","source":"from gensim import corpora\n# create a dictionary with the corpus\ncorpus = df_sorted['tokens']\ndictionary = corpora.Dictionary(corpus)\n\n# convert corpus into a bag of words\nbow = [dictionary.doc2bow(text) for text in corpus]","metadata":{"id":"ctnynPr_8h5V","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(corpus, end='\\n'+'-'*100 + '\\n')\nprint(dictionary, end='\\n'+'-'*100 + '\\n')\nprint(bow[0], end='\\n'+'-'*100 + '\\n')\n#print(dictionary.cfs, end='\\n'+'-'*100 + '\\n')\nprint(f'tokens in dictionary: {len(dictionary)}', end='\\n'+'-'*100 + '\\n')","metadata":{"id":"9pboyjWwIrlt","outputId":"23cfdc50-a408-4069-b1dc-9ac6cc62dca7","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory statistics\n\nWe count the most common words in the comic strip, plot the amount of words in each strip across the Garfield comic lifespan.","metadata":{"id":"NiV7KH_GX2x9"}},{"cell_type":"markdown","source":"### Top 100 most frequent words\n","metadata":{"id":"2-kI9i9pinX_"}},{"cell_type":"code","source":"word_id_sorted_by_frequency = sorted(dictionary.cfs, key=dictionary.cfs.get, reverse=True)\nwords_sorted_by_frequency = [dictionary[word_id] for word_id in word_id_sorted_by_frequency]\nfreqs = [dictionary.cfs[word_id] for word_id in word_id_sorted_by_frequency]","metadata":{"id":"6kG28K6TQYt1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_100_words = words_sorted_by_frequency[0:100]\ntop_100_freqs = freqs[0:100]\n\nplt.figure(figsize=(20,10))\nplt.xticks(rotation = (45), fontsize = 10, va='top', ha='right')\n\nsns.barplot(x=top_100_words, y=top_100_freqs).set(title='Top 100 word frequencies')\nplt.show()","metadata":{"id":"f7nYecvUS4sR","outputId":"9d6bca6f-4988-41ad-ceb8-5384aee9cef9","colab":{"base_uri":"https://localhost:8080/","height":814},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Document length","metadata":{"id":"YkYZLeeXXIrM"}},{"cell_type":"code","source":"# map document -> token count\nword_counts = corpus.apply(lambda doc: len(doc))\nword_counts_stats = word_counts.describe()\nprint(word_counts_stats)\n# mark shorter documents apart from the rest\nword_colors = word_counts.apply(lambda c: '#tokens $\\\\leq 2$' if c <= 2 else '#tokens $\\\\geq 2$')","metadata":{"id":"R2vqnrxqtDeg","outputId":"4d2b6f6b-29ea-4b69-e567-ea35d20321d5","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\n# plot colored word counts as scatter points\nax = sns.scatterplot(x=df_sorted.index.array, y=word_counts, hue=word_colors).set(title='Document length')\n# plot mean\nplt.hlines(xmin=0, xmax=np.max(df_sorted.index.array), y=word_counts_stats['mean'], color='red', label='mean')\nplt.xlabel('doc. index')\nplt.ylabel('#tokens')\nplt.margins(x=0, y=0)\nplt.legend()\nplt.show()\ndel ax, word_colors","metadata":{"id":"tIaFgIVOi2z7","outputId":"a1fd09f7-c2a8-4dbe-c393-10fb0e39ca9d","colab":{"base_uri":"https://localhost:8080/","height":786},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nlongest_strip_idx = np.argmax(word_counts)\nprint(df_sorted.loc[longest_strip_idx, 'text'])","metadata":{"id":"vsBsGuR8oltn","outputId":"e53ba388-56b4-444f-8fe9-fbb0daaa94a7","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Latent Semantic Analysis","metadata":{"id":"tQzJr0hc4Gze"}},{"cell_type":"markdown","source":"### LSA space creation using truncated SVD","metadata":{"id":"fCJOiVAm5WUs"}},{"cell_type":"code","source":"from gensim.models import LsiModel\n# create the lsi model/matrix factorization using the default #dimensions from gensim (k=200)\nlsi = LsiModel(bow, num_topics=200, id2word=dictionary)","metadata":{"id":"5MOIlfIxplAX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show U S V from the 200-rank approximation\n\n# left singular vectors - terms\nU = lsi.projection.u \n\n# eigenvalues from S\nS = lsi.projection.s\n\n# right singular vectors - docs\nV = gensim.matutils.corpus2dense(lsi[bow], len(lsi.projection.s)).T / lsi.projection.s\nVt = np.transpose(V)","metadata":{"id":"2RQ-sGs42H92","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Decomposition shapes:\\nU={U.shape}, S={S.shape}, Vt={Vt.shape}')","metadata":{"id":"vs5BeFDD2n8t","outputId":"98eb3f34-e8a5-4d71-8927-165c26800764","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show some topics\nlsi.show_topics()[0:10]","metadata":{"id":"-ofgKyHvqrUi","outputId":"4c1462cb-a72f-43d8-be13-5d12fe5a100b","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Queries on the LSI/LSA space\n\nQueries are a semantic extension over the boolean keyword match. The gold standard for this type of task is around 200-500 LSA dimensions (k in the low-rank approximation), so they will be performed on the 200-dim LSA space.\n\nThe dataset is not annotated with relevancy measures for certain documents.\n\nFor example, some manual queries:\n- 'sleep' semantically finds some comics in which usually Garfield sleeps (as indicated by the 'Z' onomatopoeia).\n- 'christmas' finds comics related to the holiday.\n> Garfield is an international character. Therefore, I don't even use seasons. The only holiday I recognize is Christmas. I don't use rhyming gags, plays on words, colloquialisms, in an effort to make Garfield apply to virtually any society where he may appear. In an effort to keep the gags broad, the humor general and applicable to everyone, I deal mainly with eating and sleeping. That applies to everyone, anywhere.\n\n   <cite>Jim Davis in an interview in Shapiro, W. (1982, December 12). LIVES: “The Cat That Rots the Intellect”. The Washington Post.</cite>\n\nFor more about Garfield, please read <cite>Engel, Iris B., \"It’s Garfield’s World, We Just Live in It: An Exploration of Garfield the Cat as Icon, Money\nMaker, and Beast\" (2019).\nSenior Projects Fall 2019. 3. https://digitalcommons.bard.edu/senproj_f2019/3 </cite>","metadata":{"id":"sim-kVsJwvEp"}},{"cell_type":"code","source":"# transform a doc query into the lsa space\ndoc = \"christmas\"\nvec_bow = dictionary.doc2bow(doc.lower().split())\nvec_lsi = lsi[vec_bow]  # convert the query to LSI space\n#print(vec_lsi)","metadata":{"id":"W0AKPgt-w4uk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim import similarities\nindex_lsi = similarities.MatrixSimilarity(lsi[bow])  # transform corpus (bow representation) to LSI space and index it","metadata":{"id":"1lGXtwTgxPGn","outputId":"5c053a69-e453-4c4e-aedd-87335f1e9670","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sims = index_lsi[vec_lsi]  # perform a similarity query against the corpus\n#print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples","metadata":{"id":"E21OjlJMyNqi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\nmost_relevant_docs, most_relevant_scores = list(zip(*sims_sorted))\nmost_relevant_docs = list(most_relevant_docs)\ndf_sorted.iloc[most_relevant_docs[0:5]].style","metadata":{"id":"Q0CqvlvHyVFn","outputId":"381159d0-bf69-4503-cc72-a41d4a7cd8ab","colab":{"base_uri":"https://localhost:8080/","height":310},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convenient function to make a query and get the most relevant k comic strips\ndef get_top_k_relevant_docs(query: str, k: int, index: gensim.similarities.MatrixSimilarity):\n  \"\"\"Maps the query into the latent space and computes similarity wrt each of the documents in the corpus. \n  Returns the top k documents with the highest similarity to the query.\"\"\"\n  doc = query\n  vec_bow = dictionary.doc2bow(doc.lower().split())\n  vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n  sims = index[vec_lsi]  # perform a similarity query against the corpus\n  sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n  ranked_docs, ranked_scores = list(zip(*sims_sorted))\n  return list(ranked_docs[:k]), list(ranked_scores[:k])","metadata":{"id":"BkoZEZWCBj95","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_docs, top_scores = get_top_k_relevant_docs(query='Actually, seeing the vet is not so bad', k=10, index=index_lsi)\ndf_sorted.iloc[top_docs].style","metadata":{"id":"nXsJfYhHDDXn","outputId":"b21ac331-704a-410e-dc6b-dff28ef5e7cd","colab":{"base_uri":"https://localhost:8080/","height":537},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unsupervised topic extraction in LSA space","metadata":{"id":"92Gnrigy4Nc4"}},{"cell_type":"markdown","source":"First, we will determine using the knee-point method the #topics/#dimension for a new LSI space.\n\nThen, we will analyze the found topics.","metadata":{"id":"DBavBjyj5psJ"}},{"cell_type":"markdown","source":"Plot of the eigenvalues. The higher an eigenvalue for a dimension is, the more information it carries. This is because SVD outputs the best low-rank approximation for the input matrix, in this case the term x document matrix.\n\n`kneed` is a library for finding knee points.","metadata":{"id":"U4Q2SwUz6WGM"}},{"cell_type":"code","source":"import kneed","metadata":{"id":"LLJJHBYK7feR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eigen_x = np.arange(start=0, stop=len(S))\neigen_norm = S / np.linalg.norm(S)","metadata":{"id":"rS0bNNSE7-F4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kneedle = kneed.KneeLocator(x=eigen_x, y=eigen_norm, S=1.0, curve=\"concave\", direction=\"decreasing\", online=True)\nprint(kneedle.all_knees)\nnum_topics_kneed = kneedle.knee\nprint(f'Knee point analysis suggests using k={num_topics_kneed}')","metadata":{"id":"nzrwq8ll7vAt","outputId":"2b718703-9a5c-4b7d-bfb3-ee3299987e45","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A knee point as local min of the curvature radius $k$ of the\niperbole function interpolating the data. This will be $k = \\frac{y''}{{(1+y'^2)}^\\frac{3}{2}}$","metadata":{"id":"FqDTOX1MFGGP"}},{"cell_type":"code","source":"# function to approximate derivative of an array\ndef derivative(x_data, y_data):\n    N = len(x_data)\n    delta_x = [x_data[i+1] - x_data[i] for i in range(N - 1)]\n    x_prim = [(x_data[i+1] + x_data[i]) / 2. for i in range(N - 1)]\n    y_prim = [(y_data[i+1] - y_data[i]) / delta_x[i] for i in range(N - 1)]\n    return x_prim, y_prim\n# compute 2nd derivative by calling it two times\nx_prime, y_prime = derivative(eigen_x, eigen_norm)\nx_bis, y_bis = derivative(x_prime, y_prime)\ny_prime = y_prime[:-1]\n# compute curvature according to formula\nk = np.divide(y_bis, np.power(1 + np.square(y_prime), 1.5))","metadata":{"id":"fP5x0F3i_jd1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.title('Singular values analysis')\nplt.plot(eigen_x, eigen_norm, '-o', label='normalized singular values')\nplt.plot(x_bis, k, '-x', label='curvature')\n#plt.vlines(x=list(kneedle.all_knees), ymin=ykneemin, ymax=ykneemax, linestyle='--', label='knee point from kneedle algorithm')\nvlines = [plt.axvline(x = knee, color = 'black', linestyle='--') for knee in kneedle.all_knees]\nvlines[0].set_label('knee point from kneedle algorithm')\nplt.ylabel('Magnitude')\nplt.xlabel('Singular value')\nplt.xticks(eigen_x)\nplt.xlim(0,50)\nplt.legend()\nplt.show()","metadata":{"id":"emiv6N8g6BC5","outputId":"8d588ff2-5bd4-417c-82a6-50bb14e94397","colab":{"base_uri":"https://localhost:8080/","height":775},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Or we could compute a coherence measure according to a certain number of topics.\n`CoherenceModel` does not provide stable results, though.","metadata":{"id":"SujQM6WcJDOh"}},{"cell_type":"code","source":"from gensim.models.coherencemodel import CoherenceModel\ncoherence_scores = []\n# find the coherence score with a different number of topics\nfor i in range(1, num_topics_kneed+1):\n    lsi_topics = LsiModel(bow, num_topics=i, id2word=dictionary)\n    coherence_model = CoherenceModel(model=lsi_topics, texts=df_sorted['tokens'], dictionary=dictionary, coherence='c_v', topn=10)\n    coherence_score = coherence_model.get_coherence()\n    coherence_scores.append(coherence_score)\n    print(f'Coherence score with {i} topics: {coherence_score}')","metadata":{"id":"OQJ2lAVo4qbx","outputId":"afe1fc58-fd88-4ca7-d7c6-a89f16f0b0bf","colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics_range = np.arange(1, num_topics_kneed+1)\nplt.title('Coherence scores (c_v) by #topics')\nplt.plot(topics_range, coherence_scores, '-o', label='coherence scores')\nplt.ylabel('Coherence score')\nplt.xlabel('#Topics')\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"id":"3cyCPOb-JW-7","outputId":"fd3504e6-a7a0-426d-edd1-8ffd5f3673f8","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing with LDA (latent dirichelet allocation)","metadata":{"id":"oFX8NBpLv9ns"}},{"cell_type":"code","source":"#!pip install pyLDAvis","metadata":{"id":"4VHVOCfWs8Wp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import pyLDAvis\n#import pyLDAvis.gensim_models as gensimvis\n#import pyLDAvis.gensim_models\n#from gensim.models import LdaModel\n#lda = LdaModel(bow, num_topics=2, iterations=50, passes=5)\n#pyLDAvis.enable_notebook()\n#vis = gensimvis.prepare(lda, bow, dictionary=dictionary)\n#vis","metadata":{"id":"53Luws0Ks2lD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comic strip retrieval with Deep metric learning\n\nWe will compare the previous basic LSI model with two sentence transformers.\nOne will be a pretrained one from a very large corpora of multiple text sources, the other will be a finetuned version of the pretrained one.\n\nThe finetuning will be done on 80% of the Garfield sentences, and the evaluation for all models (LSI, pretrained, finetuned) will be done on a held-out set of 20% of the sentences, and on the full dataset maybe (since this will be a common use-case).","metadata":{"id":"Yimwb6ALrOrj"}},{"cell_type":"markdown","source":"## Dataframe setup for deep metric learning\n\n","metadata":{"id":"fdyTKIfI0oXX"}},{"cell_type":"code","source":"import torch\nfrom torch import topk\nfrom torch.utils.data import Dataset, random_split\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport random\n\nfrom sentence_transformers import SentenceTransformer, util\nfrom tqdm import tqdm","metadata":{"id":"JCw565_Kw1zd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Divide each strip into sentences","metadata":{"id":"mT3CMtnKwzFi"}},{"cell_type":"code","source":"def metric_learning_setup_text(text):\n  # split around the panel\n  out = text.split('-')\n  # remove spaces\n  out = [sentence.strip(' ') for sentence in out]\n  # do not return empty strings\n  out = [sentence for sentence in out if len(sentence) >= 1]\n  return out","metadata":{"id":"0tTPFzSt-p3D","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# explode turns column with list in multiple rows with one element in each column\nsentences = df_sorted['text'].apply(metric_learning_setup_text).explode().rename(\"sentence\")\nsentences","metadata":{"id":"_m0dZxSl05EG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7ebf8e33-21f8-4b73-e3a3-73b2a93a00c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sentences = df_sorted.join(sentences)\n#df_sentences.drop(columns='tokens', inplace=True)\n\ndf_sentences.index.name = 'old index'\ndf_sentences = df_sentences.reset_index(drop=False)\ndf_sentences = df_sentences.dropna()\n# apply function to all sentences (for lsi model)\ndf_sentences['tokens'] = df_sentences['sentence'].apply(lambda x: preprocess(x))\ndf_sentences","metadata":{"id":"5bmTHXc78ev4","colab":{"base_uri":"https://localhost:8080/","height":885},"outputId":"6fe93c24-476d-48d6-f58e-ef145e31d0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify that indexes still have the correct form\n#for i in range(len(df_sentences_train)):\n#    old_index = df_sentences_train.loc[i, 'old index']\n#    strip_eval = df_sentences_train.loc[i, 'stripID']\n#    strip_sorted = df_sorted.iloc[old_index]['stripID']\n#    #print(strip, samestrip)\n#    assert(strip_eval=='strip_sorted')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### 2. Train - test split\n - Take out 500 strips for testing purposes, so that their sentences will not be trained on\n - Use remaining strips for training the `SentenceTransformer`","metadata":{"id":"yNhoq0dPPXgG"}},{"cell_type":"code","source":"# ON KAGGLE IT DOES NOT WORK\n#train_stripIDs, val_stripIDs, test_stripIDs = random_split(df_sorted['stripID'], [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n#train_stripIDs = train_stripIDs.dataset[train_stripIDs.indices].values\n#val_stripIDs = val_stripIDs.dataset[val_stripIDs.indices].values\n#test_stripIDs = test_stripIDs.dataset[test_stripIDs.indices].values","metadata":{"id":"zhT5dAfonthm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_stripIDs, val_stripIDs, test_stripIDs = np.split(df_sorted['stripID'].sample(frac=1, random_state=42),\n#                                [int(.8*len(df_sorted)), int(.9*len(df_sorted))])\ntest_stripIDs = df_sorted['stripID'].sample(n=500, random_state=42)\nlen(test_stripIDs.unique())","metadata":{"id":"283GklUrnthm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(len(train_stripIDs), len(val_stripIDs), len(test_stripIDs))\n#print(len(train_stripIDs), len(test_stripIDs))\nis_test = df_sentences['stripID'].isin(test_stripIDs)\ndf_sentences_test = df_sentences[is_test].reset_index(drop=True)\n\ndf_sentences_train = df_sentences[~is_test].reset_index(drop=True)\n\n#is_val = df_sentences['stripID'].isin(val_stripIDs)\n#df_sentences_val = df_sentences[is_val].reset_index(drop=True)\n\n#print(df_sentences_train.shape, df_sentences_val.shape, df_sentences_test.shape)\nprint(df_sentences_train.shape, df_sentences_test.shape)\n\ndf_sentences_train.loc[40:50]","metadata":{"id":"VxxYy-1NTZ2z","colab":{"base_uri":"https://localhost:8080/","height":891},"outputId":"9b2a54b0-d31b-4f2b-8bf8-5572fdb1701c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Make a dataset with previous that outputs tuples of (query, positive, negative, stripID)","metadata":{"id":"Jdewc4glDt1z"}},{"cell_type":"code","source":"from sentence_transformers import InputExample","metadata":{"id":"Px1kLkDhjGbD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GarfieldDataset(Dataset):\n  def __init__(self, dataset, mode='triplet'):\n    self.dataset = dataset\n    self.strip_number = dataset['old index'].values\n    self.sentences = dataset['sentence'].values\n    self.mode = mode\n    \n  def __len__(self):\n    return len(self.dataset)\n\n  def __getitem__(self, idx):\n    # return query, positive, negative and stripID for query index if mode is for triplet loss, otherwise \n    # only query and positive (assumed for multiplenegativeloss)\n    query = self.dataset.loc[idx, 'sentence']\n\n    # pos = another sentence in the same strip\n    positives_list = self.sentences[(self.strip_number == self.dataset.loc[idx, 'old index'])]\n    positives_list = positives_list[positives_list != query]\n    try:\n        positive = random.choice(positives_list)\n    except: positive = ''\n\n    #stripID = self.dataset.loc[idx, 'stripID']\n    example = None\n    if self.mode == 'triplet':\n        negatives_list = self.sentences[(self.strip_number != self.dataset.loc[idx, 'old index'])]\n        negatives_list = negatives_list[negatives_list != query]\n\n        negative = random.choice(negatives_list)\n        example = InputExample(texts = [query, positive, negative])\n    elif self.mode=='mn': \n        example = InputExample(texts = [query, positive])\n    #return query, positive, negative, stripID\n    return example","metadata":{"id":"TKubZHMfEGYi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data in train, test","metadata":{"id":"ts7LP5X2J5JQ"}},{"cell_type":"code","source":"train_triplet = GarfieldDataset(df_sentences_train, mode='triplet')\ntrain_mn = GarfieldDataset(df_sentences_train, mode='mn')\n#val = GarfieldDataset(df_sentences_val)\n#test = GarfieldDataset(df_sentences_test)","metadata":{"id":"q69_PAzbEYmx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Triplet example:', train_triplet.__getitem__(0).texts)\nprint('MN example:', train_mn.__getitem__(0).texts)","metadata":{"id":"d2HxtVY3KUee","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e5e369e-d915-4822-987f-da0c25e4e3f9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentence embeddings training","metadata":{"id":"09JBZUaUibCf"}},{"cell_type":"code","source":"# https://huggingface.co/blog/how-to-train-sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import losses\n#https://www.sbert.net/docs/package_reference/evaluation.html","metadata":{"id":"GZoQrNFVlHbX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consider having the .zip files in the local directory","metadata":{}},{"cell_type":"code","source":"load_finetuned_from_local = False\n\nmodel_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n# load base\nmodel_base = SentenceTransformer(model_id)\n\n# load finetuned or create from pretrained\nif load_finetuned_from_local:\n    #os.chdir(\"/kaggle/input/models/finetuned_garfield_triplet/\")\n    #shutil.unpack_archive('finetuned_garfield_triplet.zip', 'finetuned_garfield_triplet')\n    model_finetuned_triplet = SentenceTransformer('finetuned_garfield_triplet')\n    \n    #os.chdir(\"/kaggle/input/models/finetuned_garfield_mn/\")\n    #shutil.unpack_archive('finetuned_garfield_mn.zip', 'finetuned_garfield_mn')\n    #model_finetuned_mn = SentenceTransformer('finetuned_garfield_mn')\n    print('Loaded from folder.')\nelse: \n    model_finetuned_triplet = SentenceTransformer(model_id)\n    model_finetuned_mn = SentenceTransformer(model_id)","metadata":{"id":"7mxDdoHdifLM","colab":{"base_uri":"https://localhost:8080/","height":465,"referenced_widgets":["3022e9f093874a58bd89ead7c550993b","5a6f62f800cd479697fa6a63efae18c5","34e1bf93186944e1a01da4cec03e4f5b","723b0ee8bc9b466287dfb2c4148445b2","64a01d9a261f4bc082925b5f0c9deb8a","17ab097f70fc43a5b1d266cbbf9cdacf","942965aa7bd84a3994265fa26f1e5943","1894a6a9c652493bb62bcea6517b05dc","58a445147050465590fbbf225facda7b","6e9bbe0de1514f3aa9cbedeb02533da3","d7e1f92d87af4384aa58e46fa90208a1","0a7b72e9462b490987bfc60b6c6f6e2f","1eaf76e01ddf47deadfd3c29be79d5e2","685bacb165b7425b9fdf79a9466a7ad0","6e87680fe3cd4f59bd9c255ee2b08b21","120841a4c344431a9a7cd7f30f5c30cd","ba15ffa949764306a4d9be5926da2906","44fa01a1348d4bffb94920dbac45353d","db4a19ffce26420fae87d074905afc2a","e3e3f795b92a42fe9706cb1e512da533","600d8bc4c3694651934cecd8d5deccb9","875ab89b70754fd9afd2ed7d84c81dce","2ec38fa5457447a8933bf254c59a5a79","9aa38b9629644063a2348b0c3fd9e782","26d1bc3996f142db9d3a0bc3ca14e28c","47aad1d7f3a0405e99e62e972be4dc8d","4306142da8594cbfad6ab9fa7b95d404","f1fdf23faaf549b792d9c194c334a3bd","47aad87af1344aa389c3c58fc9e102e8","79ace20a68dd46c38c089a78542fbb78","beeec58764db4e04841c6ee6a574a2db","e130fe4d3112438e8bb37b5c6bf82bfd","8578cc93963a4a1e902f269426488510","2ef69facf64641bf8fcedd67ad9b10a9","36a9604cdde646178545a9c358ec00ce","0f113c6ea09f4af58b419d465b3a2da4","ea9f0ace9d8e4a118eb0f1cd409dc768","917e4626ac9b41cca2b5a1e40f04c9f7","068cf92aa9624398a653d733c877ec03","0661a0abfa754342af65ca556ddcb594","236e44b0914e4e6296cab1088ab0cbf6","b25abe531fae451dae0801cb3244ec26","243707407f93471492e9d5eea794287b","9ff5b1428ea84585ade786afbf1249b5","26a7ccde04bb47e79d0ebe3f0c5a4370","212fbdf1cd0041918a79342882d51aa7","ef2b4af02bea4abc820aae69ce702bb1","09af65616c674c0397032c2b8823fcec","1a94e3b68cf8473aba6d4894544c0663","ada78312b11148e497d1ac0365db37c4","eb4ed0a01d8645ad8d2ef65b6a4d3ce4","14ac6699e36a4792907893ca2afba8a8","b7d0f5683b3d457da6846e925dd41e54","713e9d261feb477daf539bed8ba11eb9","6d983081eda543a09b562943fd33a067","28db9ba451f240e2bd94dc423e5b2e60","71bc0e3e78c141d18e85f22c6fa95084","bdd6b6296ab64be482c810a2fb253f5b","9b99857530c942dfbb49e483accf4680","951dabf8a84743f8ac0a9ff889f6ebed","83692e4e923e4230bffa91c5f165714f","b25fccf288a244b9b7076c106ce46cb3","1136676c7259426791851d0a366bccdf","288cdb0c52b447a5b51df7990c0dff72","1f2cb5fbe0a549238c3ecc6b9a45f33e","31f2e4ea5924456b81a676d097a13cc2","fa4d1fd421794ecaac79bb996e41df11","7975b1f40d7f443d8dd085b9b140fdd8","c863e76430574cbf9256e9edc89c2752","8f3cb70dc4ce45ba86c51141cbbb86fc","c489608abddd4193b5af9c522453283b","44bfebe9150642a78c39d4c9fa9ea1ff","c82a80b68bb2498b9c6586644664a7e2","d221f91ede264ff28d329e541b0b8b71","c93d3de9254740e4a472c44f112f3ccf","cd2dfbe4c0a34032929eaabc51809c9e","10ea9620cdeb4130807002f8b2052c7b","14142c8700b446f4a0a3fc3398fe0e83","598e1d2ac6ad48b79d82057c9061457d","46a15c1703ea491bb71b26a1792aca50","969aa622d5904851be12e61006fdee8e","fade7faf38e64112b14d6358668bbc83","fd8164cdc4054403be2ae9470821ca01","5a23cac5adee49fab4b2e615a8236bf1","cf015440ddb647ec854f3985b46b9a3f","7554c06a34064a0490ae7286aec65070","b37dee74ca7f4b82935b75ccd88c0505","be935df051b7449bb9c57a31561c4b9e","f9b88c12814345e090b27970ec1d86dd","486efdebccf94e84850910399b28dfdd","11720a78b2504e7b994316e58b9a7540","ae64675f64cf472199771ac503b13ea2","5ebf5769201b44f585079c0da25a6f4a","80922a88fe71498abda892833506ebaa","e6864dc49065449f8b81588ec5641954","ad2beee5b34b4ee9924b041e523f98dc","45dd0e793af8416d9dec4f083dbe7092","425b4b16b822421da08d14954d942362","d36ca602097043b6b3d7aac7343f0db1","1d06a8b71b594f6ab766cd6ce79c4e41","df6e075d9e124d60899a40b54251f6ec","dfd8cd0bd4c442b2a081570acb2b65de","093a1b08ee654531873535d2e3389558","dab3f495f18d4a24b6741a3cbdae64cc","9f32ea0b46824c56b5520fec74c89a46","ffb6c380020147dba3803b84cb2a5629","6d0b9f31272745759e08ca32cd078069","7d8b2c896b1d4799b83698139da572c7","ff7e0110dc5348a985e803eb1b0854b4","731a914561b6450d9526c10bd46ceb11","3618f398e94a4b92aedd8688bec001f6","e2e78473935c48039df9432ef31062e9","6b15535cb6c74c14befbc3ba08328c2f","fadd5d8822ee462f9cc1afc89070ac08","a8bb5bfbfe52423da7af45872d9f9e3d","f9d5d36bbd74455cb83b9af9199ea865","8842e6e52cdc430fbfca8cefc1900f30","55d18a4da7dd4cb0af8fdeca0c10d039","7bb274d3d1a249788aed171a7a7491d1","869dbb7e54e54ec1856a63067b68e10f","874e10b834e749fb994591a560e25363","bec18e87d378499fa347efad3c607445","fe3cce2dfd1d427f869d07ba885ed3ab","c2e1928546c34d548029a8c9c08ae30e","4c4b757bfc724ff8a9d8c1ec38abdb7b","f31eb231a7b04a13a934432ee6929a4c","367b9980f7d744d0912b9523150dd49f","545dbddfbe394ac88fe0809bd38e66b1","bd1f826f25c947a684778086e066ea7f","f518b3a78b034d31beb6a0ed87441728","4d4d97482edf480facafad4667f32664","f2b7cee4236e4925b10fe6f9bb81f2e2","dbd311ada9364bba8263eb33cdf115bb","09dfecc6a98a4d038faaabdd4f484f46","0acec297761e4b229ea34cc52379ea3d","2dd749758305401c8d1b2eae5a214df5","318e1533e7c24b53923740fd7fad1491","e92de91059214586a87cd0326982de64","855ef50a40624134ba013aa2b371710c","d117a7e7b7914be983cb96485a177ceb","86acfd57a6104e15bec9055430840739","bad8016f0ace4cf99754b3fd9b46d29a","bacc94a9f86a4079be1845bef387fd0f","e815a8642ae44935ad06132d76650dc5","d6749ce9e84d4f00962ef77b09d46090","115baecd39fe44f4b21a07dedb8c56dc","599cdcc46c9749379ccf4f9ea77bc37a","9209667a8db942f79711a26f95330566","ceacc4da237d4fe79ded15de27b68b5a","d8ae8f04166e4972be122c8b6b075d13","16076595a4524419908658beeb4539a9","ad9d254292004d66906a6763d23f03c0","1d5b027a269d46c3b6502c7229949384","5802deec5bea4911b074838c6681c548"]},"outputId":"c5f2749e-79db-4c6a-c53d-f702d72b7e48","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Losses\n\n`MultipleNegativesRankingLoss`: The examples are a pair of positive sentences without a label. In this case, the positive for a sentence is another sentence from the same strip. Since the majority of all the sentences in the dataset are then negative for a query (because they come from different strips), this loss should be okay to embed closer together sentences from the same strip.\n\n`TripletLoss`: The example is a triplet (anchor, positive, negative) without classes or labels for the sentences. In this case the anchor is a sentence from strip X, positive is another sentence in strip X, and negative is a sentence from a different strip. With this loss sometimes the example are already too easy (distance anchor-positive is already smaller than the distance anchor-negative), so hard and semi-hard negative mining could be adopted to improve training.","metadata":{}},{"cell_type":"code","source":"train_loss_triplet = losses.TripletLoss(model=model_finetuned_triplet)\ntrain_loss_mn = losses.MultipleNegativesRankingLoss(model=model_finetuned_mn)\n\ntrain_dataloader_triplet = DataLoader(train_triplet, shuffle=True, batch_size=32)\ntrain_dataloader_mn = DataLoader(train_mn, shuffle=True, batch_size=32)\n\n#val_loader = DataLoader(val, batch_size=64)\n#dev_evaluator = LossEvaluator(val_loader, loss_model=train_loss, log_dir='logs/', name='dev')","metadata":{"id":"4wN2uX0FlEx8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"id":"6epk99KZnthq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finetune with Triplet Loss","metadata":{}},{"cell_type":"code","source":"#os.chdir(\"/kaggle/working\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nmodel_finetuned_triplet.fit(train_objectives=[(train_dataloader_triplet, train_loss_triplet)], epochs=10) \n\n#model_finetuned_triplet.save('finetuned_garfield_triplet')\n#shutil.make_archive('finetuned_garfield_triplet', 'zip', base_dir='finetuned_garfield_triplet')\n\ntorch.save(model_finetuned_triplet.state_dict(),'garfield_finetuned_triplet_state')","metadata":{"id":"y3Yr2Mm4lQfR","outputId":"28c37a2a-80ba-4c7e-f6ae-23555a4d82e3","colab":{"base_uri":"https://localhost:8080/","height":400,"referenced_widgets":["3203af2e2c7f40dab1ca6148b7e4deaf","0bd5732442da4664923fcd9ecec7a2c8","74b534f69c6249b797327ca34622b96b","7186c560a30d46639af36a71ef8952cf","2e30bee038de4fc48a594ce3f9c21226","ac72416bb41e46c29e9a45a885406cb5","9d81c14286e34018b0e5354f01f7f175","5dddb4652f6a45f6afdf43ba4df29292","d8d666c5de5a46e5ab969b20c46ec724","64b79d538c5c4decbc2534928043fb45","928ad680627640f08e863d91c4d48cd0","68c4f714683b4afb8c9e3273a583263a","e87502bf05d74facab05cdaa2e6ffea0","17776078d12a4c3aa1b97bbf35eaf8b2","55df1d2ee6e54cb7af7f5c34942e709f","03fa365f9ef142549e3ae1dcc9733e8d","507863faab0348bea724418583d995ba","83215425783b479f9229015a11b537f8","f7fda92d27e4490fbebc9b00155413fb","9c8a9fa19c2549b9ba4c2514dadfdeac","1ba605277722406ebade3df42cfd54c1","4994ce3e8a33432ea1730921a72cdf8e"]},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finetune with Multiple Negatives Loss","metadata":{}},{"cell_type":"code","source":"#torch.save(model_finetuned_mn.state_dict(),'garfield_finetuned_mn_state')\n#model_finetuned_triplet.load_state_dict(torch.load('garfield_finetuned_mn_state'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nmodel_finetuned_mn.fit(train_objectives=[(train_dataloader_mn, train_loss_mn)], epochs=10) \n\n#model_finetuned_mn.save('finetuned_garfield_mn')\n#shutil.make_archive('finetuned_garfield_mn', 'zip', base_dir='finetuned_garfield_mn')\ntorch.save(model_finetuned_mn.state_dict(),'garfield_finetuned_mn_state')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing Embeddings\n\nIn this section all strips in the corpus are encoded passing through the models. (GPU is needed to avoid using up a lot of time)","metadata":{"id":"pDDvdlWprtcL"}},{"cell_type":"code","source":"embeddings_base = model_base.encode(df_sorted['text'], convert_to_tensor=True)\nembeddings_finetuned_triplet = model_finetuned_triplet.encode(df_sorted['text'], convert_to_tensor=True)\nembeddings_finetuned_mn = model_finetuned_mn.encode(df_sorted['text'], convert_to_tensor=True)","metadata":{"id":"oOrNBq-zvJdd","colab":{"base_uri":"https://localhost:8080/","height":354},"outputId":"978071ab-e1ff-422c-fba5-add58a7ffad0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the closest k documents of the corpus for each query sentence based on cosine similarity on a certain model\ndef top_k_retrieval(query, df, corpus_embeddings, model, k=5, verbose=False):\n    k = min(k, len(corpus_embeddings))\n    query_embedding = model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n\n    # We use cosine-similarity and torch.topk to find the highest scores\n    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n    top_results = torch.topk(cos_scores, k=k)\n    if verbose:\n        print(top_results)\n        print(\"\\n\\n======================\\n\\n\")\n        print(\"Query:\", query)\n        print(f\"\\nTop {k} most similar sentences in corpus:\")\n\n        for score, idx in zip(top_results[0], top_results[1]):\n          print(df.iloc[int(idx)]['text'], \"(Score: {:.4f})\".format(score))\n    return top_results[0].cpu().numpy(), top_results[1].cpu().numpy().astype('int')","metadata":{"id":"UAW3ZwE9sI-B","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Query sentences:\nquery = \"Hi, there...I'm Jon Arbuckle. I'm a cartoonist, and this is my cat, Garfield.\"\ntop_k_retrieval(query, df_sorted, embeddings_base, model_base, k=10, verbose=True)","metadata":{"id":"hu__zVURt2WC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Retrieval System evaluation\n\nThe performance of `SentenceTransformer`-based retrieval is compared across different training conditions (no finetuning, finetuning with triplet loss, finetuning with multiple negatives loss) and against the `LSI` model.\n\nMeasures: Precision, Recall, F-measure up to $k$ strips retrieved, ordered by similarity wrt the query.\n\nThe test queries are the sentences from the held-out set. The `SentenceTransformers` that have trained have not seen these strips, while the `LSI` model has been created using all comics.","metadata":{"id":"-R6S10v9nths"}},{"cell_type":"code","source":"df_evaluation = df_sentences_test","metadata":{"id":"qtkb0Pu2nths","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions for query analysis","metadata":{"id":"ZB4hvBJnuuOs"}},{"cell_type":"code","source":"# from https://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S3_Evaluation.html\ndef plot_PR_curve(P_Q, R_Q, figsize=(3, 3)):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    plt.plot(R_Q, P_Q, linestyle='--', marker='o', color='k', mfc='r')\n    plt.xlim([0, 1.1])\n    plt.ylim([0, 1.1])\n    ax.set_aspect('equal', 'box')\n    plt.title('PR curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid()\n    plt.tight_layout()\n    return fig, ax\n\ndef compute_prf_metrics(I, score, I_Q):\n    \"\"\"Compute precision, recall, F-measures and other\n    evaluation metrics for document-level retrieval\n\n    Notebook: C7/C7S3_Evaluation.ipynb\n\n    Args:\n        I (np.ndarray): Array of items\n        score (np.ndarray): Array containing the score values of the times\n        I_Q (np.ndarray): Array of relevant (positive) items\n\n    Returns:\n        P_Q (float): Precision\n        R_Q (float): Recall\n        F_Q (float): F-measures sorted by rank\n        BEP (float): Break-even point\n        F_max (float): Maximal F-measure\n        P_average (float): Mean average\n        X_Q (np.ndarray): Relevance function\n        rank (np.ndarray): Array of rank values\n        I_sorted (np.ndarray): Array of items sorted by rank\n        rank_sorted (np.ndarray): Array of rank values sorted by rank\n    \"\"\"\n    # Compute rank and sort documents according to rank\n    K = len(I)\n    index_sorted = np.flip(np.argsort(score))\n    I_sorted = I[index_sorted]\n    rank = np.argsort(index_sorted) + 1\n    rank_sorted = np.arange(1, K+1)\n\n    # Compute relevance function X_Q (indexing starts with zero)\n    X_Q = np.isin(I_sorted, I_Q)\n\n    # Compute precision and recall values (indexing starts with zero)\n    M = len(I_Q)\n    P_Q = np.cumsum(X_Q) / np.arange(1, K+1)\n    R_Q = np.cumsum(X_Q) / M\n\n    # Break-even point\n    BEP = P_Q[M-1]\n    # Maximal F-measure\n    sum_PR = P_Q + R_Q\n    sum_PR[sum_PR == 0] = 1  # Avoid division by zero\n    F_Q = 2 * (P_Q * R_Q) / sum_PR\n    F_max = F_Q.max()\n    # Average precision\n    P_average = np.sum(P_Q * X_Q) / len(I_Q)\n\n    return P_Q, R_Q, F_Q, BEP, F_max, P_average, X_Q, rank, I_sorted, rank_sorted\n\ndef full_query_analysis(I, score, I_Q):\n    output = compute_prf_metrics(I, score, I_Q)\n    P_Q, R_Q, F_Q, BEP, F_max, P_average, X_Q, rank, I_sorted, rank_sorted = output\n\n    # Arrange output as tables\n    score_sorted = np.flip(np.sort(score))\n    df_query_metrics = pd.DataFrame({'Rank': rank_sorted, 'ID': I_sorted,\n                   'Score': score_sorted,\n                   'Is Relevant': X_Q, \n                   'Prec(r)': P_Q, \n                   'Rec(r)': R_Q,\n                   'F-score(r)': F_Q})\n    fig, ax = plot_PR_curve(P_Q, R_Q, figsize=(3,3))\n    ax.plot(BEP, BEP, color='green', marker='o', fillstyle='none', markersize=15)\n    ax.set_title('PR curve')\n\n    plt.show()\n\n    print('Break-even point = %.2f' % BEP)\n    print('F_max = %.2f' % F_max)\n    print('Average precision =', np.round(P_average, 5))\n    print(df_query_metrics)","metadata":{"id":"tIHqMN5snths","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functions that compute (I: Relevant documents, I_Q: the ground truth relevant document/strip, score: The similarity scores) for the different models","metadata":{"id":"hfT_KkjPuNZt"}},{"cell_type":"code","source":"def get_I_score_I_Q_for_sentence_embedder(query_idx, df, embeddings, model, k=10):\n    query = df.loc[query_idx, 'sentence']\n    I_Q = [df.loc[query_idx, 'old index']]\n    score, I = top_k_retrieval(query, df, embeddings, model, k=k, verbose=False)\n    return I, score, I_Q","metadata":{"id":"UfzqAAnEntht","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_idx = 1002\nI, score, I_Q = get_I_score_I_Q_for_sentence_embedder(query_idx, df_evaluation, embeddings_base, model_base, k=10)\nquery = df_evaluation.loc[query_idx, 'sentence']\nprint(f\"Query sentence: {query}.\\nRelevant strip: {I_Q, df_evaluation.loc[query_idx, 'stripID']}\")\nprint(f\"Full text: {df_evaluation.loc[query_idx, 'text']}\")\nfull_query_analysis(I, score, I_Q)","metadata":{"id":"oBhrZ2Ggntht","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_idx = 1002\nI, score, I_Q = get_I_score_I_Q_for_sentence_embedder(query_idx, df_evaluation, embeddings_finetuned_triplet, model_finetuned_triplet, k=10)\nquery = df_evaluation.loc[query_idx, 'sentence']\nprint(f\"Query sentence: {query}.\\nRelevant strip: {I_Q, df_evaluation.loc[query_idx, 'stripID']}\")\nprint(f\"Full text: {df_evaluation.loc[query_idx, 'text']}\")\nfull_query_analysis(I, score, I_Q)","metadata":{"id":"LMPBWUU1zonB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_I_score_I_Q_for_LSI_model(query_idx, df, embeddings, model, k=10):\n  \"\"\"Maps the query into the latent space and computes similarity wrt each of the documents in the corpus. \n  Returns the top k documents with the highest similarity to the query.\"\"\"\n  doc = df.loc[query_idx,'tokens']\n  vec_bow = dictionary.doc2bow(doc)\n  vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n  sims = index_lsi[vec_lsi]  # perform a similarity query against the corpus\n  sims_sorted = sorted(enumerate(sims), key=lambda item: -item[1])\n  ranked_docs, ranked_scores = list(zip(*sims_sorted))\n  I = np.array(ranked_docs[:k])\n  score = np.array(ranked_scores[:k])\n  I_Q = [df.loc[query_idx, 'old index']]\n  return I, score, I_Q","metadata":{"id":"2Zc7axchuEQC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query_idx = 1002\nprint('Query:', df_evaluation.loc[query_idx,'sentence'])\nI, score, I_Q = get_I_score_I_Q_for_LSI_model(query_idx, df_evaluation, index_lsi, lsi, k=10)\nprint('Retrieved: ', df_sorted.iloc[I]['text'])\nfull_query_analysis(I, score, I_Q)","metadata":{"id":"gvRN-bnpuDux","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparison of results across the considered models","metadata":{"id":"SqwheFVOuc15"}},{"cell_type":"code","source":"def compute_performances_on_whole_dataset(k, df, retrieval_function, kwargs):\n    \"\"\"Compute precision, recall, F-measures and other\n    evaluation metrics for document-level retrieval on all queries in dataframe for rank up to k.\n\n    Args:\n        df: Dataframe with columns \"sentence\", 'old index'\n        embeddings: Embeddings of the corpus documents.\n        model: model with '.encode' method to encode queries with.\n        k: max number of documents retrieved based on score.\n\n    Returns:\n        Dataframe with MAP, Precision, Recall, Fscore@1,5,10 columms.\"\"\"\n\n    avg_precision = np.zeros(k)\n    avg_recall = np.zeros(k)\n    avg_fscore = np.zeros(k)\n    avg_MAP = 0\n    \n    for query_idx in tqdm(range(len(df))):\n        # compute metrics for each query, and accumulate them\n        #score, I = top_k_retrieval(df['sentence'][query_idx], embeddings, model, k=k, verbose=False)\n        #I_Q = [df.loc[query_idx, 'old index']]\n        I, score, I_Q = retrieval_function(query_idx = query_idx, **kwargs)\n        output = compute_prf_metrics(I, score, I_Q)\n        P_Q, R_Q, F_Q, BEP, F_max, P_average, X_Q, rank, I_sorted, rank_sorted = output\n\n        avg_precision += P_Q\n        avg_recall += R_Q\n        avg_fscore += F_Q\n        avg_MAP += P_average\n        \n    # divide by n. of queries made\n    avg_precision /= len(df)\n    avg_recall /= len(df)\n    avg_fscore /= len(df)\n    avg_MAP /= len(df)\n    \n    results = pd.DataFrame({'MAP' : avg_MAP, \n                            'Precision@k' : [avg_precision],\n                            'Recall@k' : [avg_recall],\n                            'Fscore@k' : [avg_fscore]})\n    #for ki in range(k):\n    #    results[f'Precision@{ki+1}'] = avg_precision[ki] \n    #    results[f'Recall@{ki+1}'] = avg_recall[ki]\n    #    results[f'Fscore@{ki+1}'] = avg_fscore[ki]\n    #results.loc[0, f'Precision@k'] = avg_precision[0]\n    #results.loc[0, f'Recall@k'] = avg_precision[0]\n    #results.loc[0, f'Fscore@k'] = avg_precision[0]\n    \n    return results","metadata":{"id":"b-n4C63antht","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = [model_base, model_finetuned_triplet, model_finetuned_mn, lsi]\nembeddings = [embeddings_base, embeddings_finetuned_triplet, embeddings_finetuned_mn, index_lsi]\nnames = ['base', 'finetuned_triplet', 'finetuned_mn', 'lsi']\n#TEST\n#models = [model_base, model_finetuned, lsi]\n#names = ['base', 'finetuned_triplet', 'lsi']\n#embeddings = [corpus_embeddings_base, corpus_embeddings_finetuned, index_lsi]\n\ndef create_results_df(df_evaluation):\n    partial_results = []\n    # compute metrics for each model\n    for model, embedding, name in zip(models, embeddings, names):\n        print(f'Evaluating {name}')\n        if not(name == \"lsi\"):\n            partial_result = compute_performances_on_whole_dataset(k=10, \n                                                     df=df_evaluation,\n                                                     retrieval_function=get_I_score_I_Q_for_sentence_embedder,\n                                                     kwargs={'df':df_evaluation,\n                                                           'embeddings': embedding,\n                                                           'model': model})\n        else:\n            partial_result = compute_performances_on_whole_dataset(k=10, \n                                                     df=df_evaluation[:100],\n                                                     retrieval_function=get_I_score_I_Q_for_LSI_model,\n                                                     kwargs={'df':df_evaluation,\n                                                           'embeddings': embedding,\n                                                           'model': model})\n        # name\n        partial_result.rename(index={0: name}, inplace=True)\n        # add\n        partial_results.append(partial_result)\n    results = pd.concat(partial_results)\n    return results","metadata":{"id":"pA5k3rvMntht","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regarding the task of strip retrieval from a sentence, we compute the metrics wrt to multiple sets of queries.\n\n- Sentences in the whole dataset. We will see that the finedtuned version using multiple negatives loss outperforms the base sentence encoder.\n- Held-out sentences in the test dataset. We can observe that the general pretrained-version is still better at generalizing.","metadata":{}},{"cell_type":"code","source":"#results_general = create_results_df(df_evaluation = df_sentences_train)\nresults_test = create_results_df(df_evaluation = df_sentences_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_results(x, y, label, filled_marker_style):\n  plt.plot(x, y, fillstyle = \"none\", label=label, **filled_marker_style)\n\ndef plot_metrics_multiple_models(dataframe, title, metric): #the metric you want to print: \"f1-score\", \"precision\", \"recall\"\n  colors = ['blue', 'orange', 'green', 'red']\n  filled_marker_style = dict(marker='x', linestyle='-', markersize=6,\n                           color='blue',\n                           markerfacecolor='white',\n                           #markerfacecoloralt='tab:blue',\n                           markeredgecolor='blue')\n  \n  series = dataframe[metric]\n  k_range = range(1, len(series[0])+1)\n\n  plt.figure(figsize=(10,4))\n  plt.grid(linestyle=\"--\")\n  plt.suptitle(title)\n  plt.ylim(0.0, 1.0)\n  plt.ylabel(metric)\n  plt.xlabel('k (rank)')\n  plt.yticks(np.arange(0, 1.1, step=0.1 ))\n  plt.xticks(k_range)\n\n  for i, (name, values) in enumerate(zip(series.index, series.values)):\n    filled_marker_style['color'] = colors[i]\n    filled_marker_style['markeredgecolor'] = colors[i]\n    plot_results(k_range, values, name, filled_marker_style)\n  \n  plt.legend()\n  plt.plot()","metadata":{"id":"i_uzxfhbntht","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = \"Results comparison on all sentences\"\nfor index in results.index:\n    print(f\"Mean Average Precision for model {index}: {results_general['MAP'][index]:.3f}\")\nplot_metrics_multiple_models(results_general, title + \" - Fscore\", 'Fscore@k')\nplot_metrics_multiple_models(results_general, title +\" - Recall\", 'Recall@k')\nplot_metrics_multiple_models(results_general, title +\" - Precision\", 'Precision@k')\n","metadata":{"id":"9ZkRygMhnthu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = \"Results comparison on test sentences\"\nfor index in results_test.index:\n    print(f\"Mean Average Precision for model {index}: {results_test['MAP'][index]:.3f}\")\nplot_metrics_multiple_models(results_test, title + \" - Fscore\", 'Fscore@k')\nplot_metrics_multiple_models(results_test, title +\" - Recall\", 'Recall@k')\nplot_metrics_multiple_models(results_test, title +\" - Precision\", 'Precision@k')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Top2Vec topic mining","metadata":{"id":"lVGZ0C3WxxNC"}},{"cell_type":"markdown","source":"### Installs","metadata":{"id":"CCcjc3-Ax8TI"}},{"cell_type":"code","source":"#!pip install top2vec\n!pip install top2vec[sentence_encoders]","metadata":{"id":"p21qnqMYx2EQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from top2vec import Top2Vec","metadata":{"id":"Pcij11LhyB1G","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using top2vec tutorial","metadata":{"id":"b4YI2s0sx-Cy"}},{"cell_type":"code","source":"documents = df_sorted['text'].values","metadata":{"id":"j-NDaK6syul9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"documents","metadata":{"id":"QyXXkea02CjU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Top2Vec(documents, min_count=1, ngram_vocab=True, tokenizer=preprocess, embedding_model='universal-sentence-encoder', workers=100, speed='deep-learn')\n","metadata":{"id":"aMiyGIw1xzOD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('top2vec_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load('top2vec_model')","metadata":{"id":"YqADjWDRzAHx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_num_topics()","metadata":{"id":"qOFclT16zQTk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_topic_sizes()","metadata":{"id":"Au6VhOUKzTqI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.get_topics()","metadata":{"id":"j2sKZe1gzal8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.query_documents('My name is jon arbuckle i am a cartoonist', num_docs=10)","metadata":{"id":"QaPZe4mvzqMT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.query_documents('I can''t sleep.', num_docs=10)","metadata":{"id":"re4hACMX8Lg-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"monday\", 'mondays'], num_topics=5)\n#for topic in topic_nums:\n#    model.generate_topic_wordcloud(topic)","metadata":{"id":"sbYnJbi08nYP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text completion and generation\n","metadata":{"id":"U4sYJZWsIGgq"}},{"cell_type":"markdown","source":"The hope is to train a generator model that can generate sentences to put in garfield strips.\n\nNotable examples online are [Garkov](http://joshmillard.com/garkov/), GPT3, ecc.\n\nIDK I am going to fine-tune a DistilRoberta to build a language model.\n\nHOW TO EVALUATE during training: CROSS ENTROPY LOSS BASED ON TRAIN/VAL/SPLIT\n\nMAKE A TRIGRAM MODEL ALSO (?)\n\nFINAL TASK: \n\n\ntest set: blanks the last phrase, the model predicts it. -> metric is the f1 based on the overlap between the prediction and the ground truth...","metadata":{"id":"3AggrxtAIP0B"}},{"cell_type":"markdown","source":"### Dataframe setup for sentence completion\n\nWe need to\n- tokenize into a numerical format the bow.\n- mask some parts for training our language model.","metadata":{"id":"qpuUrCouSuG9"}},{"cell_type":"code","source":"# for training, discard rows that have empty strips.\ndf_gen = df_sorted[df_sorted['tokens'].map(len) > 0].copy().reset_index(drop=True)\n\n# map text in dataframe to numerical indexes\ndf_gen['tokens_int'] = df_gen['tokens'].apply(lambda token_list: [dictionary.token2id[token] for token in token_list])","metadata":{"id":"Ns8lqq5fSxim","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_gen","metadata":{"id":"vS4hxNQRaaNq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRY THIS\n\nhttps://www.sbert.net/index.html\n\nhttps://stats.stackexchange.com/questions/274478/understanding-input-shape-parameter-in-lstm-with-keras\nx_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1))","metadata":{"id":"s2nOHzMWl07R","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models import FastText\nimport tensorflow as tf\nimport keras\nfrom keras import Model, Sequential\nfrom keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n\n# prepare text for keras neural network\n\nmax_len = 30\nsequence_docs = tf.keras.preprocessing.sequence.pad_sequences(df_gen['tokens_int'], maxlen=max_len, value=-1)\n\nprint(sequence_docs[0:10])\n\n# define a keras model and load the pretrained fasttext weights matrix\nmodel = Sequential()\n\ninp = Input(shape=(max_len,1))\nmodel.add(inp)\n\n#model.add(Embedding(100, 10, input_length=max_len))\n\nlstm_layer = LSTM(100, return_sequences=False, input_shape=(max_len,1))\nmodel.add(lstm_layer)\n\n#model.add(Dense(1))\n\nmodel.build()\nmodel.summary()\n\noutput = model.predict(sequence_docs)","metadata":{"id":"wa1Eds4sWlIw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.shape","metadata":{"id":"IZst9aL1ksAj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d","metadata":{"id":"zbIfUxdaQazz","trusted":true},"execution_count":null,"outputs":[]}]}